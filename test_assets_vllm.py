#!/usr/bin/env python
# test_assets_vllm_pipe_topk.py   â€”â€”  ä¸€æ¬¡ç”Ÿæˆ k æ ‡ç­¾ï¼Œé»˜è®¤ k=5

import argparse, json, re
from collections import defaultdict

import pandas as pd
from vllm import LLM, SamplingParams
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å‚æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser(description='èµ„äº§åç§°åˆ†ç±» (ä¸€æ¬¡ç”Ÿæˆ k æ ‡ç­¾)')
parser.add_argument('--model_path', type=str,
    default='/hpc2hdd/home/qxiao183/models/Qwen2.5-7B-Instruct')
parser.add_argument('--gpu_util', type=float, default=0.9)
parser.add_argument('--input', type=str, default='./æ–°å›½æ ‡å›ºå®šèµ„äº§.xlsx')
parser.add_argument('--text_col', type=str, default='èµ„äº§åç§°')
parser.add_argument('--label_col', type=str, default='æ–°å›½æ ‡åˆ†ç±»')
parser.add_argument('--batch_size', type=int, default=32)
parser.add_argument('--prompt_style', type=str, default='labels',
    choices=['labels', 'labels_examples'])
parser.add_argument('--examples_per_label', type=int, default=1)
parser.add_argument('--top_k', type=int, default=5,            # â˜… é»˜è®¤ 5
    help='è¦æ±‚æ¨¡å‹ä¸€æ¬¡è¿”å›çš„å€™é€‰æ ‡ç­¾ä¸ªæ•°')
parser.add_argument('--out_csv', type=str, default='./assets_pred_pipe.csv')
args = parser.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¯»å–å¹¶å»é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.read_excel(args.input)[[args.text_col, args.label_col]].dropna()
df = df.drop_duplicates(subset=[args.text_col]).reset_index(drop=True)
print(f'âœ” å»é‡åæ ·æœ¬æ•°: {len(df)}')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ„é€  label_prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
labels = df[args.label_col].unique().tolist()
if args.prompt_style == 'labels':
    label_prompt = "ã€".join(labels)
else:
    sample_dict = defaultdict(list)
    for _, r in df.iterrows():
        if len(sample_dict[r[args.label_col]]) < args.examples_per_label:
            sample_dict[r[args.label_col]].append(r[args.text_col])
        if all(len(v) >= args.examples_per_label for v in sample_dict.values()):
            break
    label_prompt = "\n".join(
        f"{l}ï¼ˆä¾‹ï¼š{'ï¼›'.join(sample_dict[l])}ï¼‰" for l in labels
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– vLLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print('â³ åŠ è½½æ¨¡å‹ â€¦')
llm = LLM(model=args.model_path, gpu_memory_utilization=args.gpu_util)
sampling = SamplingParams(
    max_tokens=32,
    temperature=0.0,
    stop=["\n", "ã€‚", "ï¼Œ", ",", "ã€‚"]
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt æ¨¡æ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
prompt_tpl = (
    "ä½ æ˜¯ä¸€ä½å›ºå®šèµ„äº§åˆ†ç±»ä¸“å®¶ã€‚\n"
    "è¯·ä»ä¸‹åˆ—å¯é€‰ç±»åˆ«ä¸­é€‰å‡ºå¯èƒ½æ€§æœ€é«˜çš„ {k} ä¸ªï¼Œç”¨ç«–çº¿â€œ|â€åˆ†éš”ï¼ŒæŒ‰å¯èƒ½æ€§ä»é«˜åˆ°ä½æ’åˆ—ã€‚\n"
    "ä¸å¾—è¾“å‡ºåˆ—è¡¨ä¹‹å¤–çš„ç±»åˆ«ï¼Œä¹Ÿä¸è¦è¾“å‡ºå¤šä½™å­—ç¬¦ã€‚\n"
    "å¯é€‰ç±»åˆ«ï¼š\n{label_prompt}\n"
    "èµ„äº§åç§°ï¼š{asset}\n"
    "ç±»åˆ«ï¼š"
)

def make_prompts(batch):
    return [
        prompt_tpl.format(k=args.top_k, label_prompt=label_prompt, asset=x)
        for x in batch
    ]

def split_labels(gen: str, k: int) -> list[str]:
    """æŠŠæ¨¡å‹ç”Ÿæˆçš„â€œæ ‡ç­¾1|æ ‡ç­¾2|â€¦â€æ‹†æˆåˆ—è¡¨ï¼›ä¸è¶³ k ç”¨ç©ºå­—ç¬¦ä¸²è¡¥é½ã€‚"""
    parts = re.split(r"[|ã€,ï¼Œ\s]+", gen.strip())
    parts = [p.strip() for p in parts if p.strip()]
    return (parts + [""] * k)[:k]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¨ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
candidates, top1 = [], []
print('ğŸš€ æ¨ç† â€¦')
for i in tqdm(range(0, len(df), args.batch_size)):
    batch_txt = df[args.text_col].iloc[i:i+args.batch_size].tolist()
    outs = llm.generate(make_prompts(batch_txt), sampling)
    for o in outs:
        gen = o.outputs[0].text
        cand = split_labels(gen, args.top_k)
        candidates.append(cand)
        top1.append(cand[0])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¯„ä¼° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
truth = df[args.label_col].tolist()
acc_k = sum(t in cand for t, cand in zip(truth, candidates)) / len(df)
acc1  = accuracy_score(truth, top1)

print(f"\nğŸ” Accuracy@{args.top_k}: {acc_k:.4f}")
print(f"ğŸ” Top-1 Accuracy   : {acc1:.4f}\n")
print("â€”â€” Top-1 classification report â€”â€”")
print(classification_report(truth, top1, digits=4))

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¿å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# df['å€™é€‰æ ‡ç­¾'] = ["|".join(c) for c in candidates]
# df['top1'] = top1
# df.to_csv(args.out_csv, index=False, encoding='utf-8')
# print(f'âœ… ç»“æœå·²ä¿å­˜åˆ°: {args.out_csv}')

# id2label = {i: l for i, l in enumerate(sorted(labels))}
# with open(args.out_csv.replace('.csv', '_id2label.json'), 'w', encoding='utf-8') as f:
#     json.dump(id2label, f, ensure_ascii=False, indent=2)
# print('âœ… id2label.json å·²ç”Ÿæˆ')